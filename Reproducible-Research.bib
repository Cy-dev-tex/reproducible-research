
@article{boettigerIntroductionRockerDocker2017a,
  langid = {english},
  title = {An {{Introduction}} to {{Rocker}}: {{Docker Containers}} for {{R}}},
  volume = {9},
  issn = {2073-4859},
  url = {https://journal.r-project.org/archive/2017/RJ-2017-065/index.html},
  doi = {10.32614/RJ-2017-065},
  shorttitle = {An {{Introduction}} to {{Rocker}}},
  abstract = {We describe the Rocker project, which provides a widely-used suite of Docker images with customized R environments for particular tasks. We discuss how this suite is organized, and how these tools can increase portability, scaling, reproducibility, and convenience of R users and developers.},
  number = {2},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  urldate = {2019-08-14},
  date = {2017},
  pages = {527},
  author = {Boettiger, Carl and Eddelbuettel, Dirk},
  file = {/home/aaron/Zotero/storage/MXHKN3JA/Boettiger and Eddelbuettel - 2017 - An Introduction to Rocker Docker Containers for R.pdf}
}

@online{revolutionanalyticsReproducibilityUsingFixed2019,
  title = {Reproducibility: {{Using Fixed CRAN Repository Snapshots}} . {{MRAN}}},
  url = {https://mran.microsoft.com/documents/rro/reproducibility},
  journaltitle = {Microsoft R Application Network},
  urldate = {2019-08-14},
  date = {2019},
  author = {{Revolution Analytics}},
  file = {/home/aaron/Zotero/storage/BUY2HVLY/reproducibility.html}
}

@online{dockerinc.DockerHub2019,
  title = {Docker {{Hub}}},
  url = {https://hub.docker.com/},
  urldate = {2019-08-14},
  date = {2019},
  author = {{Docker Inc.}},
  file = {/home/aaron/Zotero/storage/GNXRF37A/hub.docker.com.html}
}

@article{hodgesStatisticalMethodsResearch2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.08381},
  primaryClass = {stat},
  title = {Statistical Methods Research Done as Science Rather than Mathematics},
  url = {http://arxiv.org/abs/1905.08381},
  abstract = {This paper is about how we study statistical methods. As an example, it uses the random regressions model, in which the intercept and slope of cluster-specific regression lines are modeled as a bivariate random effect. Maximizing this model's restricted likelihood often gives a boundary value for the random effect correlation or variances. We argue that this is a problem; that it is a problem because our discipline has little understanding of how contemporary models and methods map data to inferential summaries; that we lack such understanding, even for models as simple as this, because of a near-exclusive reliance on mathematics as a means of understanding; and that math alone is no longer sufficient. We then argue that as a discipline, we can and should break open our black-box methods by mimicking the five steps that molecular biologists commonly use to break open Nature's black boxes: design a simple model system, formulate hypotheses using that system, test them in experiments on that system, iterate as needed to reformulate and test hypotheses, and finally test the results in an "in vivo" system. We demonstrate this by identifying conditions under which the random-regressions restricted likelihood is likely to be maximized at a boundary value. Resistance to this approach seems to arise from a view that it lacks the certainty or intellectual heft of mathematics, perhaps because simulation experiments in our literature rarely do more than measure a new method's operating characteristics in a small range of situations. We argue that such work can make useful contributions including, as in molecular biology, the findings themselves and sometimes the designs used in the five steps; that these contributions have as much practical value as mathematical results; and that therefore they merit publication as much as the mathematical results our discipline esteems so highly.},
  urldate = {2019-08-14},
  date = {2019-05-20},
  keywords = {Statistics - Other Statistics},
  author = {Hodges, James S.},
  file = {/home/aaron/Zotero/storage/C8BQJADJ/Hodges - 2019 - Statistical methods research done as science rathe.pdf;/home/aaron/Zotero/storage/X8C6CJQR/1905.html}
}

@article{ruleTenSimpleRules2019,
  langid = {english},
  title = {Ten Simple Rules for Writing and Sharing Computational Analyses in {{Jupyter Notebooks}}},
  volume = {15},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007},
  doi = {10.1371/journal.pcbi.1007007},
  number = {7},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2019-07-25},
  pages = {e1007007},
  keywords = {Analysts,Computer and information sciences,Computer hardware,Data processing,Ecosystems,Graphical user interfaces,Metadata,Reproducibility},
  author = {Rule, Adam and Birmingham, Amanda and Zuniga, Cristal and Altintas, Ilkay and Huang, Shih-Cheng and Knight, Rob and Moshiri, Niema and Nguyen, Mai H. and Rosenthal, Sara Brin and Pérez, Fernando and Rose, Peter W.},
  file = {/home/aaron/Zotero/storage/Q92J4JHM/Rule et al. - 2019 - Ten simple rules for writing and sharing computati.pdf;/home/aaron/Zotero/storage/SXYJ4MVC/article.html}
}

@article{barbaHardRoadReproducibility2016,
  langid = {english},
  title = {The Hard Road to Reproducibility},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/354/6308/142},
  doi = {10.1126/science.354.6308.142},
  abstract = {Early in my Ph.D. studies, my supervisor assigned me the task of running computer code written by a previous student who was graduated and gone. It was hell. I had to sort through many different versions of the code, saved in folders with a mysterious numbering scheme. There was no documentation and scarcely an explanatory comment in the code itself. It took me at least a year to run the code reliably, and more to get results that reproduced those in my predecessor's thesis. Now that I run my own lab, I make sure that my students don't have to go through that.

![Figure][1]{$<$}/img{$>$}

ILLUSTRATION: ROBERT NEUBECKER

{$>$} “My students and I continuously discuss and perfect our standards.” 

In 2012, I wrote a manifesto in which I committed to best practices for reproducibility. Today, a new student arriving in my group finds all of our research code in tidy repositories, where every change is recorded automatically. Version control is our essential technology for record keeping and collaboration. Whenever we publish a paper, we create a “reproducibility package,” deposited online, which includes the data sets and all the code that is needed to recreate the analyses and figures. These are the practices that work for us as computational scientists, but the principles behind them apply regardless of discipline.

It takes new students some time to learn how to work to these standards, but we have documentation and training materials to make it as painless as possible. My students don't resent investing their time in this. They know that practices like ours are crucial for the integrity of the scientific endeavor. They also appreciate that our approach will help them show potential future employers that they are careful, conscientious researchers.

I am pleased when our group is recognized for our high standards in other people's writings, and when we are invited to speak about these practices at meetings. But we've found we still have a lot to learn about what it takes for research, even when done to high standards of reproducibility, to be replicated. A couple years ago, we published a paper applying computational fluid dynamics to the aerodynamics of flying snakes. More recently, I asked a new student to replicate the findings of that paper, both as a training opportunity and to help us choose which code to use in future research. Replicating a published study is always difficult—there are just so many conditions that need to be matched and details that can't be overlooked—but I thought this case was relatively straightforward. The data were available. The whole analysis was open for inspection. The additional details were documented in the supplementary materials. It was the very definition of reproducible research.

Three years of work and hundreds of runs with four different codes taught us just how many ways there are to go wrong! Failing to record the version of any piece of software or hardware, overlooking a single parameter, or glossing over a restriction on how to use another researcher's code can lead you astray.

We've found that we can only achieve the necessary level of reliability and transparency by automating every step. Manual actions are replaced by scripts or logged into files. Plots are made only via code, not with a graphical user interface. Every result, including those from failed experiments, is documented. Every step of the way, we want to anticipate what another researcher might need to either reproduce our results (run our code with our data) or replicate them (independently arrive at the same findings).

About 150 years ago, Louis Pasteur demonstrated how experiments can be conducted reproducibly—and the value of doing so. His research had many skeptics at first, but they were persuaded by his claims after they reproduced his results, using the methods he had recorded in keen detail. In computational science, we are still learning to be in his league. My students and I continuously discuss and perfect our standards, and we share our reproducibility practices with our community in the hopes that others will adopt similar ideals. Yes, conducting our research to these standards takes time and effort—and maybe our papers are slower to be published. But they're less likely to be wrong.

 [1]: pending:yes},
  number = {6308},
  journaltitle = {Science},
  urldate = {2019-08-17},
  date = {2016-10-07},
  pages = {142-142},
  author = {Barba, Lorena A.},
  file = {/home/aaron/Zotero/storage/XGVLL8X2/Barba - 2016 - The hard road to reproducibility.pdf;/home/aaron/Zotero/storage/Q7TF9V3L/tab-article-info.html},
  eprinttype = {pmid},
  eprint = {27846503}
}

@article{pengReproducibleResearchComputational2011,
  langid = {english},
  title = {Reproducible {{Research}} in {{Computational Science}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/334/6060/1226},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  number = {6060},
  journaltitle = {Science},
  urldate = {2019-08-17},
  date = {2011-12-02},
  pages = {1226-1227},
  author = {Peng, Roger D.},
  file = {/home/aaron/Zotero/storage/MBXRAUVA/Peng - 2011 - Reproducible Research in Computational Science.pdf;/home/aaron/Zotero/storage/DBIAQEGC/1226.html},
  eprinttype = {pmid},
  eprint = {22144613}
}

@article{wilsonGoodEnoughPractices2017,
  langid = {english},
  title = {Good Enough Practices in Scientific Computing},
  volume = {13},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510},
  doi = {10.1371/journal.pcbi.1005510},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  number = {6},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2017-06-22},
  pages = {e1005510},
  keywords = {Computer software,Control systems,Data management,Data processing,Programming languages,Reproducibility,Software tools,Source code},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  file = {/home/aaron/Zotero/storage/F6Q8I65R/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/home/aaron/Zotero/storage/8CR2LD9F/article.html}
}

@article{sandveTenSimpleRules2013,
  langid = {english},
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
  doi = {10.1371/journal.pcbi.1003285},
  number = {10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2013-10-24},
  pages = {e1003285},
  keywords = {Archives,Computer and information sciences,Computer applications,Habits,Replication studies,Reproducibility,Sequence analysis,Source code},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  file = {/home/aaron/Zotero/storage/P3QILX6Y/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf;/home/aaron/Zotero/storage/8ECJH7ZU/article.html}
}

@article{taschukTenSimpleRules2017,
  langid = {english},
  title = {Ten Simple Rules for Making Research Software More Robust},
  volume = {13},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005412},
  doi = {10.1371/journal.pcbi.1005412},
  abstract = {Software produced for research, published and otherwise, suffers from a number of common problems that make it difficult or impossible to run outside the original institution or even off the primary developer’s computer. We present ten simple rules to make such software robust enough to be run by anyone, anywhere, and thereby delight your users and collaborators.},
  number = {4},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2017-04-13},
  pages = {e1005412},
  keywords = {Bioinformatics,Computer software,Operating systems,Reproducibility,Sequence alignment,Software development,Software engineering,Software tools},
  author = {Taschuk, Morgan and Wilson, Greg},
  file = {/home/aaron/Zotero/storage/7SD2P7D6/Taschuk and Wilson - 2017 - Ten simple rules for making research software more.pdf;/home/aaron/Zotero/storage/VWZI8A6E/article.html}
}

@article{novellaContainerbasedBioinformaticsPachyderm2019,
  langid = {english},
  title = {Container-Based Bioinformatics with {{Pachyderm}}},
  volume = {35},
  issn = {1367-4803},
  url = {https://academic.oup.com/bioinformatics/article/35/5/839/5068160},
  doi = {10.1093/bioinformatics/bty699},
  abstract = {AbstractMotivation.  Computational biologists face many challenges related to data size, and they need to manage complicated analyses often including multiple s},
  number = {5},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2019-08-17},
  date = {2019-03-01},
  pages = {839-846},
  author = {Novella, Jon Ander and Emami Khoonsari, Payam and Herman, Stephanie and Whitenack, Daniel and Capuccini, Marco and Burman, Joachim and Kultima, Kim and Spjuth, Ola},
  file = {/home/aaron/Zotero/storage/6YY6VRNY/Novella et al. - 2019 - Container-based bioinformatics with Pachyderm.pdf;/home/aaron/Zotero/storage/Z6WTE8Y4/5068160.html}
}

@article{ivieReproducibilityScientificComputing2018,
  title = {Reproducibility in {{Scientific Computing}}},
  volume = {51},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/3186266},
  doi = {10.1145/3186266},
  abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today’s computational work is adequately reproducible. In principle, it should be possible to specify a computation to sufficient detail that anyone should be able to reproduce it exactly. But in practice, there are fundamental, technical, and social barriers to doing so. The many objectives and meanings of reproducibility are discussed within the context of scientific computing. Technical barriers to reproducibility are described, extant approaches surveyed, and open areas of research are identified.},
  number = {3},
  journaltitle = {ACM Comput. Surv.},
  urldate = {2019-08-17},
  date = {2018-07},
  pages = {63:1--63:36},
  keywords = {computational science,replicability,Reproducibility,reproducible,scientific computing,scientific workflow,scientific workflows,workflow,workflows},
  author = {Ivie, Peter and Thain, Douglas}
}

@article{clyburne-sherinComputationalReproducibilityContainers2018,
  title = {Computational {{Reproducibility}} via {{Containers}} in {{Social Psychology}}},
  url = {https://osf.io/mf82t},
  abstract = {NOTE: Revised version currently under submission at Met-psychology. Under peer review at Meta-Psychology, submission number MP2018.892, link: https://osf.io/ps5ru/. Anyone can participate in peer review by sending the editor an email, or through discussion on social media. The preferred way of open commenting, however, is to use the hypothes.is integration at PsyArXiv and directly comment on this preprint. Editor: Rickard Carlsson, rickard.carlsson@lnu.seWebsite: https://open.lnu.se/index.php/metapsychology ABSTRACT: Scientific progress relies on the replication and reuse of research. However, despite an emerging culture of sharing code and data in psychology, the research practices needed to achieve computational reproducibility -- the quality of a research project entailing the provision of sufficient code, data and documentation to allow an independent researcher to re-obtain the project's results -- are not widely adopted. Historically, the ability to share and reuse computationally reproducible research was technically challenging and time-consuming. One welcome development on this front is the advent of containers, a technology intended to facilitate code sharing for software development. Containers, however, remain technically demanding and imperfectly suited for research applications. This editorial argues that the use of containers adapted for research can help foster a culture of reproducibiliy in psychology research. We will illustrate this by introducing Code Ocean, an online computational reproducibility platform. (Disclaimer: the authors work for Code Ocean.)},
  urldate = {2019-08-17},
  date = {2018-02-16},
  author = {Clyburne-Sherin, April and Fei, Xu and Green, Seth Ariel},
  file = {/home/aaron/Zotero/storage/ZCS6KPN2/Clyburne-Sherin et al. - Computational Reproducibility via Containers in Ps.pdf},
  doi = {10.31234/osf.io/mf82t}
}

@article{hardwicketome.DataAvailabilityReusability,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  volume = {5},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.180448},
  doi = {10.1098/rsos.180448},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (‘analytic reproducibility’). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  number = {8},
  journaltitle = {Royal Society Open Science},
  shortjournal = {Royal Society Open Science},
  urldate = {2019-08-17},
  pages = {180448},
  author = {{Hardwicke Tom E.} and {Mathur Maya B.} and {MacDonald Kyle} and {Nilsonne Gustav} and {Banks George C.} and {Kidwell Mallory C.} and {Hofelich Mohr Alicia} and {Clayton Elizabeth} and {Yoon Erica J.} and {Henry Tessler Michael} and {Lenne Richie L.} and {Altman Sara} and {Long Bria} and {Frank Michael C.}},
  file = {/home/aaron/Zotero/storage/4AP6VEES/Hardwicke Tom E. et al. - Data availability, reusability, and analytic repro.pdf;/home/aaron/Zotero/storage/QJZFV9EE/rsos.html}
}

@article{askrenUsingMakeReproducible2016,
  langid = {english},
  title = {Using {{Make}} for {{Reproducible}} and {{Parallel Neuroimaging Workflow}} and {{Quality}}-{{Assurance}}},
  volume = {10},
  issn = {1662-5196},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2016.00002/full},
  doi = {10.3389/fninf.2016.00002},
  abstract = {The contribution of this paper is to describe how we can program neuroimaging workflow using Make, a software development tool designed for describing how to build executables from source files. We show that we can achieve many of the features of more sophisticated neuroimaging pipeline systems, including reproducibility, parallelization, fault tolerance, and quality assurance reports. We suggest that Make represents a large step towards these features with only a modest increase in programming demands over shell scripts. This approach reduces the technical skill and time required to write, debug, and maintain neuroimaging workflows in a dynamic environment, where pipelines are often modified to accommodate new best practices or to study the effect of alternative preprocessing steps, and where the underlying packages change frequently. This paper has a comprehensive accompanying manual with lab practicals and examples (see Supplemental Materials) and all data, scripts and makefiles necessary to run the practicals and examples are available in the “makepipelines” project at NITRC.},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front. Neuroinform.},
  urldate = {2019-08-17},
  date = {2016},
  keywords = {Neuroimaging methods,neuroimaging pipelines,Quality Assurance,reproducibility,workflow},
  author = {Askren, Mary K. and McAllister-Day, Trevor K. and Koh, Natalie and Mestre, Zoé and Dines, Jennifer N. and Korman, Benjamin A. and Melhorn, Susan J. and Peterson, Daniel J. and Peverill, Matthew and Qin, Xiaoyan and Rane, Swati D. and Reilly, Melissa A. and Reiter, Maya A. and Sambrook, Kelly A. and Woelfer, Karl A. and Grabowski, Thomas J. and Madhyastha, Tara M.},
  file = {/home/aaron/Zotero/storage/8Y3973GK/Askren et al. - 2016 - Using Make for Reproducible and Parallel Neuroimag.pdf}
}


