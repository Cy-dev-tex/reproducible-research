
@article{boettigerIntroductionRockerDocker2017a,
  langid = {english},
  title = {An {{Introduction}} to {{Rocker}}: {{Docker Containers}} for {{R}}},
  volume = {9},
  issn = {2073-4859},
  url = {https://journal.r-project.org/archive/2017/RJ-2017-065/index.html},
  doi = {10.32614/RJ-2017-065},
  shorttitle = {An {{Introduction}} to {{Rocker}}},
  abstract = {We describe the Rocker project, which provides a widely-used suite of Docker images with customized R environments for particular tasks. We discuss how this suite is organized, and how these tools can increase portability, scaling, reproducibility, and convenience of R users and developers.},
  number = {2},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  urldate = {2019-08-14},
  date = {2017},
  pages = {527},
  author = {Boettiger, Carl and Eddelbuettel, Dirk},
  file = {/home/aaron/Zotero/storage/MXHKN3JA/Boettiger and Eddelbuettel - 2017 - An Introduction to Rocker Docker Containers for R.pdf}
}

@Manual{liftr,
  title = {liftr: Containerize R Markdown Documents for Continuous Reproducibility},
  author = {Nan Xiao},
  year = {2019},
  note = {R package version 0.9.2},
  url = {https://CRAN.R-project.org/package=liftr},
}

@Manual{rrtools,
  title = {rrtools: Creates a Reproducible Research Compendium},
author = {Ben Marwick},
  year = {2019},
  note = {R package version 0.1.0},
  url = {https://github.com/benmarwick/rrtools},
}

@online{revolutionanalyticsReproducibilityUsingFixed2019,
  title = {Reproducibility: {{Using Fixed CRAN Repository Snapshots}} . {{MRAN}}},
  url = {https://mran.microsoft.com/documents/rro/reproducibility},
  journaltitle = {Microsoft R Application Network},
  urldate = {2019-08-14},
  date = {2019},
  author = {{Revolution Analytics}},
  file = {/home/aaron/Zotero/storage/BUY2HVLY/reproducibility.html}
}

@online{dockerinc.DockerHub2019,
  title = {Docker {{Hub}}},
  url = {https://hub.docker.com/},
  urldate = {2019-08-14},
  date = {2019},
  author = {{Docker Inc.}},
  file = {/home/aaron/Zotero/storage/GNXRF37A/hub.docker.com.html}
}

@Manual{redoc,
  title = {redoc: Reversible Reproducible Documents},
  author = {Noam Ross},
year = {2019},
  note = {R package version 0.1.0.9000},
  url = {https://github.com/noamross/redoc},
}

@article{hodgesStatisticalMethodsResearch2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.08381},
  primaryClass = {stat},
  title = {Statistical Methods Research Done as Science Rather than Mathematics},
  url = {http://arxiv.org/abs/1905.08381},
  abstract = {This paper is about how we study statistical methods. As an example, it uses the random regressions model, in which the intercept and slope of cluster-specific regression lines are modeled as a bivariate random effect. Maximizing this model's restricted likelihood often gives a boundary value for the random effect correlation or variances. We argue that this is a problem; that it is a problem because our discipline has little understanding of how contemporary models and methods map data to inferential summaries; that we lack such understanding, even for models as simple as this, because of a near-exclusive reliance on mathematics as a means of understanding; and that math alone is no longer sufficient. We then argue that as a discipline, we can and should break open our black-box methods by mimicking the five steps that molecular biologists commonly use to break open Nature's black boxes: design a simple model system, formulate hypotheses using that system, test them in experiments on that system, iterate as needed to reformulate and test hypotheses, and finally test the results in an "in vivo" system. We demonstrate this by identifying conditions under which the random-regressions restricted likelihood is likely to be maximized at a boundary value. Resistance to this approach seems to arise from a view that it lacks the certainty or intellectual heft of mathematics, perhaps because simulation experiments in our literature rarely do more than measure a new method's operating characteristics in a small range of situations. We argue that such work can make useful contributions including, as in molecular biology, the findings themselves and sometimes the designs used in the five steps; that these contributions have as much practical value as mathematical results; and that therefore they merit publication as much as the mathematical results our discipline esteems so highly.},
  urldate = {2019-08-14},
  date = {2019-05-20},
  keywords = {Statistics - Other Statistics},
  author = {Hodges, James S.},
  file = {/home/aaron/Zotero/storage/C8BQJADJ/Hodges - 2019 - Statistical methods research done as science rathe.pdf;/home/aaron/Zotero/storage/X8C6CJQR/1905.html}
}

@article{ruleTenSimpleRules2019,
  langid = {english},
  title = {Ten Simple Rules for Writing and Sharing Computational Analyses in {{Jupyter Notebooks}}},
  volume = {15},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007},
  doi = {10.1371/journal.pcbi.1007007},
  number = {7},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2019-07-25},
  pages = {e1007007},
  keywords = {Analysts,Computer and information sciences,Computer hardware,Data processing,Ecosystems,Graphical user interfaces,Metadata,Reproducibility},
  author = {Rule, Adam and Birmingham, Amanda and Zuniga, Cristal and Altintas, Ilkay and Huang, Shih-Cheng and Knight, Rob and Moshiri, Niema and Nguyen, Mai H. and Rosenthal, Sara Brin and Pérez, Fernando and Rose, Peter W.},
  file = {/home/aaron/Zotero/storage/Q92J4JHM/Rule et al. - 2019 - Ten simple rules for writing and sharing computati.pdf;/home/aaron/Zotero/storage/SXYJ4MVC/article.html}
}

@article{barbaHardRoadReproducibility2016,
  langid = {english},
  title = {The Hard Road to Reproducibility},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/354/6308/142},
  doi = {10.1126/science.354.6308.142},
  abstract = {Early in my Ph.D. studies, my supervisor assigned me the task of running computer code written by a previous student who was graduated and gone. It was hell. I had to sort through many different versions of the code, saved in folders with a mysterious numbering scheme. There was no documentation and scarcely an explanatory comment in the code itself. It took me at least a year to run the code reliably, and more to get results that reproduced those in my predecessor's thesis. Now that I run my own lab, I make sure that my students don't have to go through that.

![Figure][1]{$<$}/img{$>$}

ILLUSTRATION: ROBERT NEUBECKER

{$>$} “My students and I continuously discuss and perfect our standards.”

In 2012, I wrote a manifesto in which I committed to best practices for reproducibility. Today, a new student arriving in my group finds all of our research code in tidy repositories, where every change is recorded automatically. Version control is our essential technology for record keeping and collaboration. Whenever we publish a paper, we create a “reproducibility package,” deposited online, which includes the data sets and all the code that is needed to recreate the analyses and figures. These are the practices that work for us as computational scientists, but the principles behind them apply regardless of discipline.

It takes new students some time to learn how to work to these standards, but we have documentation and training materials to make it as painless as possible. My students don't resent investing their time in this. They know that practices like ours are crucial for the integrity of the scientific endeavor. They also appreciate that our approach will help them show potential future employers that they are careful, conscientious researchers.

I am pleased when our group is recognized for our high standards in other people's writings, and when we are invited to speak about these practices at meetings. But we've found we still have a lot to learn about what it takes for research, even when done to high standards of reproducibility, to be replicated. A couple years ago, we published a paper applying computational fluid dynamics to the aerodynamics of flying snakes. More recently, I asked a new student to replicate the findings of that paper, both as a training opportunity and to help us choose which code to use in future research. Replicating a published study is always difficult—there are just so many conditions that need to be matched and details that can't be overlooked—but I thought this case was relatively straightforward. The data were available. The whole analysis was open for inspection. The additional details were documented in the supplementary materials. It was the very definition of reproducible research.

Three years of work and hundreds of runs with four different codes taught us just how many ways there are to go wrong! Failing to record the version of any piece of software or hardware, overlooking a single parameter, or glossing over a restriction on how to use another researcher's code can lead you astray.

We've found that we can only achieve the necessary level of reliability and transparency by automating every step. Manual actions are replaced by scripts or logged into files. Plots are made only via code, not with a graphical user interface. Every result, including those from failed experiments, is documented. Every step of the way, we want to anticipate what another researcher might need to either reproduce our results (run our code with our data) or replicate them (independently arrive at the same findings).

About 150 years ago, Louis Pasteur demonstrated how experiments can be conducted reproducibly—and the value of doing so. His research had many skeptics at first, but they were persuaded by his claims after they reproduced his results, using the methods he had recorded in keen detail. In computational science, we are still learning to be in his league. My students and I continuously discuss and perfect our standards, and we share our reproducibility practices with our community in the hopes that others will adopt similar ideals. Yes, conducting our research to these standards takes time and effort—and maybe our papers are slower to be published. But they're less likely to be wrong.

 [1]: pending:yes},
  number = {6308},
  journaltitle = {Science},
  urldate = {2019-08-17},
  date = {2016-10-07},
  pages = {142-142},
  author = {Barba, Lorena A.},
  file = {/home/aaron/Zotero/storage/XGVLL8X2/Barba - 2016 - The hard road to reproducibility.pdf;/home/aaron/Zotero/storage/Q7TF9V3L/tab-article-info.html},
  eprinttype = {pmid},
  eprint = {27846503}
}

@article{pengReproducibleResearchComputational2011,
  langid = {english},
  title = {Reproducible {{Research}} in {{Computational Science}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/334/6060/1226},
  doi = {10.1126/science.1213847},
  abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  number = {6060},
  journaltitle = {Science},
  urldate = {2019-08-17},
  date = {2011-12-02},
  pages = {1226-1227},
  author = {Peng, Roger D.},
  file = {/home/aaron/Zotero/storage/MBXRAUVA/Peng - 2011 - Reproducible Research in Computational Science.pdf;/home/aaron/Zotero/storage/DBIAQEGC/1226.html},
  eprinttype = {pmid},
  eprint = {22144613}
}

@article{wilsonGoodEnoughPractices2017,
  langid = {english},
  title = {Good Enough Practices in Scientific Computing},
  volume = {13},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510},
  doi = {10.1371/journal.pcbi.1005510},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  number = {6},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2017-06-22},
  pages = {e1005510},
  keywords = {Data processing,Reproducibility,Computer software,Control systems,Data management,Programming languages,Software tools,Source code},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  file = {/home/aaron/Zotero/storage/F6Q8I65R/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf;/home/aaron/Zotero/storage/8CR2LD9F/article.html}
}

@article{sandveTenSimpleRules2013,
  langid = {english},
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285},
  doi = {10.1371/journal.pcbi.1003285},
  number = {10},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2013-10-24},
  pages = {e1003285},
  keywords = {Computer and information sciences,Reproducibility,Source code,Archives,Computer applications,Habits,Replication studies,Sequence analysis},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  file = {/home/aaron/Zotero/storage/P3QILX6Y/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf;/home/aaron/Zotero/storage/8ECJH7ZU/article.html}
}

@article{taschukTenSimpleRules2017,
  langid = {english},
  title = {Ten Simple Rules for Making Research Software More Robust},
  volume = {13},
  issn = {1553-7358},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005412},
  doi = {10.1371/journal.pcbi.1005412},
  abstract = {Software produced for research, published and otherwise, suffers from a number of common problems that make it difficult or impossible to run outside the original institution or even off the primary developer’s computer. We present ten simple rules to make such software robust enough to be run by anyone, anywhere, and thereby delight your users and collaborators.},
  number = {4},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2019-08-17},
  date = {2017-04-13},
  pages = {e1005412},
  keywords = {Reproducibility,Computer software,Software tools,Bioinformatics,Operating systems,Sequence alignment,Software development,Software engineering},
  author = {Taschuk, Morgan and Wilson, Greg},
  file = {/home/aaron/Zotero/storage/7SD2P7D6/Taschuk and Wilson - 2017 - Ten simple rules for making research software more.pdf;/home/aaron/Zotero/storage/VWZI8A6E/article.html}
}

@article{novellaContainerbasedBioinformaticsPachyderm2019,
  langid = {english},
  title = {Container-Based Bioinformatics with {{Pachyderm}}},
  volume = {35},
  issn = {1367-4803},
  url = {https://academic.oup.com/bioinformatics/article/35/5/839/5068160},
  doi = {10.1093/bioinformatics/bty699},
  abstract = {AbstractMotivation.  Computational biologists face many challenges related to data size, and they need to manage complicated analyses often including multiple s},
  number = {5},
  journaltitle = {Bioinformatics},
  shortjournal = {Bioinformatics},
  urldate = {2019-08-17},
  date = {2019-03-01},
  pages = {839-846},
  author = {Novella, Jon Ander and Emami Khoonsari, Payam and Herman, Stephanie and Whitenack, Daniel and Capuccini, Marco and Burman, Joachim and Kultima, Kim and Spjuth, Ola},
  file = {/home/aaron/Zotero/storage/6YY6VRNY/Novella et al. - 2019 - Container-based bioinformatics with Pachyderm.pdf;/home/aaron/Zotero/storage/Z6WTE8Y4/5068160.html}
}

@article{ivieReproducibilityScientificComputing2018,
  title = {Reproducibility in {{Scientific Computing}}},
  volume = {51},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/3186266},
  doi = {10.1145/3186266},
  abstract = {Reproducibility is widely considered to be an essential requirement of the scientific process. However, a number of serious concerns have been raised recently, questioning whether today’s computational work is adequately reproducible. In principle, it should be possible to specify a computation to sufficient detail that anyone should be able to reproduce it exactly. But in practice, there are fundamental, technical, and social barriers to doing so. The many objectives and meanings of reproducibility are discussed within the context of scientific computing. Technical barriers to reproducibility are described, extant approaches surveyed, and open areas of research are identified.},
  number = {3},
  journaltitle = {ACM Comput. Surv.},
  urldate = {2019-08-17},
  date = {2018-07},
  pages = {63:1--63:36},
  keywords = {Reproducibility,computational science,replicability,reproducible,scientific computing,scientific workflow,scientific workflows,workflow,workflows},
  author = {Ivie, Peter and Thain, Douglas}
}

@article{clyburne-sherinComputationalReproducibilityContainers2018,
  title = {Computational {{Reproducibility}} via {{Containers}} in {{Social Psychology}}},
  url = {https://osf.io/mf82t},
  abstract = {NOTE: Revised version currently under submission at Met-psychology. Under peer review at Meta-Psychology, submission number MP2018.892, link: https://osf.io/ps5ru/. Anyone can participate in peer review by sending the editor an email, or through discussion on social media. The preferred way of open commenting, however, is to use the hypothes.is integration at PsyArXiv and directly comment on this preprint. Editor: Rickard Carlsson, rickard.carlsson@lnu.seWebsite: https://open.lnu.se/index.php/metapsychology ABSTRACT: Scientific progress relies on the replication and reuse of research. However, despite an emerging culture of sharing code and data in psychology, the research practices needed to achieve computational reproducibility -- the quality of a research project entailing the provision of sufficient code, data and documentation to allow an independent researcher to re-obtain the project's results -- are not widely adopted. Historically, the ability to share and reuse computationally reproducible research was technically challenging and time-consuming. One welcome development on this front is the advent of containers, a technology intended to facilitate code sharing for software development. Containers, however, remain technically demanding and imperfectly suited for research applications. This editorial argues that the use of containers adapted for research can help foster a culture of reproducibiliy in psychology research. We will illustrate this by introducing Code Ocean, an online computational reproducibility platform. (Disclaimer: the authors work for Code Ocean.)},
  urldate = {2019-08-17},
  date = {2018-02-16},
  author = {Clyburne-Sherin, April and Fei, Xu and Green, Seth Ariel},
  file = {/home/aaron/Zotero/storage/ZCS6KPN2/Clyburne-Sherin et al. - Computational Reproducibility via Containers in Ps.pdf},
  doi = {10.31234/osf.io/mf82t}
}

@article{hardwicketome.DataAvailabilityReusability,
  title = {Data Availability, Reusability, and Analytic Reproducibility: Evaluating the Impact of a Mandatory Open Data Policy at the Journal {{Cognition}}},
  volume = {5},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.180448},
  doi = {10.1098/rsos.180448},
  shorttitle = {Data Availability, Reusability, and Analytic Reproducibility},
  abstract = {Access to data is a critical feature of an efficient, progressive and ultimately self-correcting scientific ecosystem. But the extent to which in-principle benefits of data sharing are realized in practice is unclear. Crucially, it is largely unknown whether published findings can be reproduced by repeating reported analyses upon shared data (‘analytic reproducibility’). To investigate this, we conducted an observational evaluation of a mandatory open data policy introduced at the journal Cognition. Interrupted time-series analyses indicated a substantial post-policy increase in data available statements (104/417, 25\% pre-policy to 136/174, 78\% post-policy), although not all data appeared reusable (23/104, 22\% pre-policy to 85/136, 62\%, post-policy). For 35 of the articles determined to have reusable data, we attempted to reproduce 1324 target values. Ultimately, 64 values could not be reproduced within a 10\% margin of error. For 22 articles all target values were reproduced, but 11 of these required author assistance. For 13 articles at least one value could not be reproduced despite author assistance. Importantly, there were no clear indications that original conclusions were seriously impacted. Mandatory open data policies can increase the frequency and quality of data sharing. However, suboptimal data curation, unclear analysis specification and reporting errors can impede analytic reproducibility, undermining the utility of data sharing and the credibility of scientific findings.},
  number = {8},
  journaltitle = {Royal Society Open Science},
  shortjournal = {Royal Society Open Science},
  urldate = {2019-08-17},
  pages = {180448},
  author = {{Hardwicke Tom E.} and {Mathur Maya B.} and {MacDonald Kyle} and {Nilsonne Gustav} and {Banks George C.} and {Kidwell Mallory C.} and {Hofelich Mohr Alicia} and {Clayton Elizabeth} and {Yoon Erica J.} and {Henry Tessler Michael} and {Lenne Richie L.} and {Altman Sara} and {Long Bria} and {Frank Michael C.}},
  file = {/home/aaron/Zotero/storage/4AP6VEES/Hardwicke Tom E. et al. - Data availability, reusability, and analytic repro.pdf;/home/aaron/Zotero/storage/QJZFV9EE/rsos.html}
}

@article{askrenUsingMakeReproducible2016,
  langid = {english},
  title = {Using {{Make}} for {{Reproducible}} and {{Parallel Neuroimaging Workflow}} and {{Quality}}-{{Assurance}}},
  volume = {10},
  issn = {1662-5196},
  url = {https://www.frontiersin.org/articles/10.3389/fninf.2016.00002/full},
  doi = {10.3389/fninf.2016.00002},
  abstract = {The contribution of this paper is to describe how we can program neuroimaging workflow using Make, a software development tool designed for describing how to build executables from source files. We show that we can achieve many of the features of more sophisticated neuroimaging pipeline systems, including reproducibility, parallelization, fault tolerance, and quality assurance reports. We suggest that Make represents a large step towards these features with only a modest increase in programming demands over shell scripts. This approach reduces the technical skill and time required to write, debug, and maintain neuroimaging workflows in a dynamic environment, where pipelines are often modified to accommodate new best practices or to study the effect of alternative preprocessing steps, and where the underlying packages change frequently. This paper has a comprehensive accompanying manual with lab practicals and examples (see Supplemental Materials) and all data, scripts and makefiles necessary to run the practicals and examples are available in the “makepipelines” project at NITRC.},
  journaltitle = {Frontiers in Neuroinformatics},
  shortjournal = {Front. Neuroinform.},
  urldate = {2019-08-17},
  date = {2016},
  keywords = {workflow,Neuroimaging methods,neuroimaging pipelines,Quality Assurance,reproducibility},
  author = {Askren, Mary K. and McAllister-Day, Trevor K. and Koh, Natalie and Mestre, Zoé and Dines, Jennifer N. and Korman, Benjamin A. and Melhorn, Susan J. and Peterson, Daniel J. and Peverill, Matthew and Qin, Xiaoyan and Rane, Swati D. and Reilly, Melissa A. and Reiter, Maya A. and Sambrook, Kelly A. and Woelfer, Karl A. and Grabowski, Thomas J. and Madhyastha, Tara M.},
  file = {/home/aaron/Zotero/storage/8Y3973GK/Askren et al. - 2016 - Using Make for Reproducible and Parallel Neuroimag.pdf}
}

@article{gentlemanStatisticalAnalysesReproducible2007,
  title = {Statistical {{Analyses}} and {{Reproducible Research}}},
  volume = {16},
  issn = {1061-8600},
  url = {https://doi.org/10.1198/106186007X178663},
  doi = {10.1198/106186007X178663},
  abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents—including figures, tables, and so on—can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or “source” document from which one can generate different views in the form of traditional, derived documents for different audiences.We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection.The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
  number = {1},
  journaltitle = {Journal of Computational and Graphical Statistics},
  urldate = {2019-08-20},
  date = {2007-03-01},
  pages = {1-23},
  keywords = {Compendium,Dynamic documents,Literate programming,Markup language,Perl,Python,R},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  file = {/home/aaron/Zotero/storage/3SGY946G/Gentleman and Lang - 2007 - Statistical Analyses and Reproducible Research.pdf;/home/aaron/Zotero/storage/Q9FLZ859/106186007X178663.html}
}

@article{marwickPackagingDataAnalytical2018,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  volume = {72},
  issn = {0003-1305},
  url = {https://doi.org/10.1080/00031305.2017.1375986},
  doi = {10.1080/00031305.2017.1375986},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  number = {1},
  journaltitle = {The American Statistician},
  urldate = {2019-08-20},
  date = {2018-01-02},
  pages = {80-88},
  keywords = {Computational science,Data science,Open source software,Reproducible research},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  file = {/home/aaron/Zotero/storage/DNQFZ2R6/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf;/home/aaron/Zotero/storage/G7DFFNLC/00031305.2017.html}
}

@article{bryanExcuseMeYou2018,
  title = {Excuse {{Me}}, {{Do You Have}} a {{Moment}} to {{Talk About Version Control}}?},
  volume = {72},
  issn = {0003-1305},
  url = {https://doi.org/10.1080/00031305.2017.1399928},
  doi = {10.1080/00031305.2017.1399928},
  abstract = {Data analysis, statistical research, and teaching statistics have at least one thing in common: these activities all produce many files! There are data files, source code, figures, tables, prepared reports, and much more. Most of these files evolve over the course of a project and often need to be shared with others, for reading or edits, as a project unfolds. Without explicit and structured management, project organization can easily descend into chaos, taking time away from the primary work and reducing the quality of the final product. This unhappy result can be avoided by repurposing tools and workflows from the software development world, namely, distributed version control. This article describes the use of the version control system Git and the hosting site GitHub for statistical and data scientific workflows. Special attention is given to projects that use the statistical language R and, optionally, R Markdown documents. Supplementary materials include an annotated set of links to step-by-step tutorials, real world examples, and other useful learning resources. Supplementary materials for this article are available online.},
  number = {1},
  journaltitle = {The American Statistician},
  urldate = {2019-08-20},
  date = {2018-01-02},
  pages = {20-27},
  keywords = {Data science,Git,GitHub,R language; R Markdown,Reproducibility,Workflow},
  author = {Bryan, Jennifer},
  file = {/home/aaron/Zotero/storage/UGZFF7KW/Bryan - 2018 - Excuse Me, Do You Have a Moment to Talk About Vers.pdf;/home/aaron/Zotero/storage/PPV8W3DN/00031305.2017.html}
}

article{stodden2016enhancing,
  title={Enhancing reproducibility for computational methods},
  author={Stodden, Victoria and McNutt, Marcia and Bailey, David H and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A and Ioannidis, John PA and Taufer, Michela},
  journal={Science},
  volume={354},
  number={6317},
  pages={1240--1241},
  year={2016},
  publisher={American Association for the Advancement of Science}
}

@Manual{gohel2019officer,
  title = {officer: Manipulation of Microsoft Word and PowerPoint Documents},
  author = {David Gohel},
  year = {2019},
  note = {R package version 0.3.5},
  url = {https://CRAN.R-project.org/package=officer},
}

@article{stoddenEnhancingReproducibilityComputational2016,
  langid = {english},
  title = {Enhancing Reproducibility for Computational Methods},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/354/6317/1240},
  doi = {10.1126/science.aah6168},
  abstract = {Data, code, and workflows should be available and cited
Data, code, and workflows should be available and cited},
  number = {6317},
  journaltitle = {Science},
  urldate = {2019-08-23},
  date = {2016-12-09},
  pages = {1240-1241},
  author = {Stodden, Victoria and McNutt, Marcia and Bailey, David H. and Deelman, Ewa and Gil, Yolanda and Hanson, Brooks and Heroux, Michael A. and Ioannidis, John P. A. and Taufer, Michela},
  file = {/home/aaron/Zotero/storage/P4BYX664/1240.html},
  eprinttype = {pmid},
  eprint = {27940837}
}

@article{fosterOpenScienceFramework2017,
  title = {Open {{Science Framework}} ({{OSF}})},
  volume = {105},
  issn = {1536-5050},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5370619/},
  doi = {10.5195/jmla.2017.88},
  number = {2},
  journaltitle = {Journal of the Medical Library Association : JMLA},
  shortjournal = {J Med Libr Assoc},
  urldate = {2019-08-26},
  date = {2017-04},
  pages = {203-206},
  author = {Foster, Erin D. and Deardorff, Ariel},
  file = {/home/aaron/Zotero/storage/YJVQDVJL/Foster and Deardorff - 2017 - Open Science Framework (OSF).pdf},
  eprinttype = {pmid},
  eprint = {null},
  pmcid = {PMC5370619}
}

@Manual{workflowr,
  title = {workflowr: A Framework for Reproducible and Collaborative Data Science},
  author = {John Blischak and Peter Carbonetto and Matthew Stephens},
  year = {2019},
  note = {R package version 1.4.0.9001},
  url = {https://github.com/jdblischak/workflowr},
}

@Article{drake,
  title = {The drake R package: a pipeline toolkit for reproducibility and high-performance computing},
  author = {William Michael Landau},
  journal = {Journal of Open Source Software},
  year = {2018},
  volume = {3},
  number = {21},
  url = {https://doi.org/10.21105/joss.00550},
}

@Manual{holepunch,
  title = {holepunch: Configure Your R Project for 'binderhub'},
  author = {Karthik Ram},
  year = {2019},
  note = {R package version 0.1.25.9000},
  url = {https://github.com/karthik/holepunch},
}

@Manual{packrat,
  title = {packrat: A Dependency Management System for Projects and their R Package Dependencies},
  author = {Kevin Ushey and Jonathan McPherson and Joe Cheng and Aron Atkins and JJ Allaire},
  year = {2018},
  note = {R package version 0.5.0},
  url = {https://CRAN.R-project.org/package=packrat},
}

@Manual{reprex,
  title = {reprex: Prepare Reproducible Example Code via the Clipboard},
  author = {Jennifer Bryan and Jim Hester and David Robinson and Hadley Wickham},
  year = {2019},
  note = {R package version 0.3.0},
  url = {https://CRAN.R-project.org/package=reprex},
}


@article{yenniDevelopingModernData2018,
  langid = {english},
  title = {Developing a Modern Data Workflow for Living Data},
  url = {https://www.biorxiv.org/content/10.1101/344804v1},
  doi = {10.1101/344804},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Data management and publication are core components of the research process. An emerging challenge that has received limited attention in biology is managing, working with, and providing access to data under continual active collection. “Living data” present unique challenges in quality assurance and control, data publication, archiving, and reproducibility. We developed a living data workflow for a long-term ecological study that addresses many of the challenges associated with managing this type of data. We do this by leveraging existing tools to: 1) perform quality assurance and control; 2) import, restructure, version, and archive data; 3) rapidly publish new data in ways that ensure appropriate credit to all contributors; and 4) automate most steps in the data pipeline to reduce the time and effort required by researchers. The workflow uses two tools from software development, version control and continuous integration, to create a modern data management system that automates the pipeline.{$<$}/p{$>$}},
  journaltitle = {bioRxiv},
  urldate = {2019-08-29},
  date = {2018-06-12},
  pages = {344804},
  author = {Yenni, Glenda M. and Christensen, Erica M. and Bledsoe, Ellen K. and Supp, Sarah R. and Diaz, Renata M. and White, Ethan P. and Ernest, S. K. Morgan},
  file = {/home/aaron/Zotero/storage/WWRZ4D6X/Yenni et al. - 2018 - Developing a modern data workflow for living data.pdf;/home/aaron/Zotero/storage/DUBGLVEJ/344804v1.html}
}

@article{beaulieu-jonesReproducibilityComputationalWorkflows2017,
  langid = {english},
  title = {Reproducibility of Computational Workflows Is Automated Using Continuous Analysis},
  volume = {35},
  issn = {1546-1696},
  url = {https://www.nature.com/articles/nbt.3780},
  doi = {10.1038/nbt.3780},
  abstract = {The application of continuous integration, an approach common in software development, enables the automatic reproduction of computational analyses.},
  number = {4},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  urldate = {2019-08-29},
  date = {2017-04},
  pages = {342-346},
  author = {Beaulieu-Jones, Brett K. and Greene, Casey S.},
  file = {/home/aaron/Zotero/storage/4KA99I8G/Beaulieu-Jones and Greene - 2017 - Reproducibility of computational workflows is auto.pdf;/home/aaron/Zotero/storage/W3HY9SIY/nbt.html}
}

@article{kurtzerSingularityScientificContainers2017,
  langid = {english},
  title = {Singularity: {{Scientific}} Containers for Mobility of Compute},
  volume = {12},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459},
  doi = {10.1371/journal.pone.0177459},
  shorttitle = {Singularity},
  abstract = {Here we present Singularity, software developed to bring containers and reproducibility to scientific computing. Using Singularity containers, developers can work in reproducible environments of their choosing and design, and these complete environments can easily be copied and executed on other platforms. Singularity is an open source initiative that harnesses the expertise of system and software engineers and researchers alike, and integrates seamlessly into common workflows for both of these groups. As its primary use case, Singularity brings mobility of computing to both users and HPC centers, providing a secure means to capture and distribute software and compute environments. This ability to create and deploy reproducible environments across these centers, a previously unmet need, makes Singularity a game changing development for computational science.},
  number = {5},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-08-29},
  date = {2017-05-11},
  pages = {e0177459},
  keywords = {Computer software,Open source software,Operating systems,Research validity,Software design,Software development,Software tools,Tar},
  author = {Kurtzer, Gregory M. and Sochat, Vanessa and Bauer, Michael W.}
}

@article{perkelToolkitDataTransparency2018,
  langid = {english},
  title = {A Toolkit for Data Transparency Takes Shape},
  volume = {560},
  url = {http://www.nature.com/articles/d41586-018-05990-5},
  doi = {10.1038/d41586-018-05990-5},
  abstract = {A simple software toolset can help to ease the pain of reproducing computational analyses.},
  journaltitle = {Nature},
  urldate = {2019-08-29},
  date = {2018-08-20},
  pages = {513-515},
  author = {Perkel, Jeffrey M.},
  file = {/home/aaron/Zotero/storage/CDWTVY6G/Perkel - 2018 - A toolkit for data transparency takes shape.pdf;/home/aaron/Zotero/storage/HTWSV7JT/d41586-018-05990-5.html}
}

@article{perkelPioneeringLivecodeArticle2019,
  langid = {english},
  title = {Pioneering ‘Live-Code’ Article Allows Scientists to Play with Each Other’s Results},
  volume = {567},
  url = {http://www.nature.com/articles/d41586-019-00724-7},
  doi = {10.1038/d41586-019-00724-7},
  abstract = {The goal is to use software functionality to make science easier to test, reproduce and build on.},
  journaltitle = {Nature},
  urldate = {2019-08-29},
  date = {2019-02-28},
  pages = {17-18},
  author = {Perkel, Jeffrey M.},
  file = {/home/aaron/Zotero/storage/9S74TNKU/Perkel - 2019 - Pioneering ‘live-code’ article allows scientists t.pdf;/home/aaron/Zotero/storage/EGKIS475/d41586-019-00724-7.html}
}

@article{piccoloToolsTechniquesComputational2016,
  title = {Tools and Techniques for Computational Reproducibility},
  volume = {5},
  issn = {2047-217X},
  url = {https://doi.org/10.1186/s13742-016-0135-4},
  doi = {10.1186/s13742-016-0135-4},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed—and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  number = {1},
  journaltitle = {GigaScience},
  shortjournal = {GigaScience},
  urldate = {2019-08-29},
  date = {2016-07-11},
  pages = {30},
  author = {Piccolo, Stephen R. and Frampton, Michael B.},
  file = {/home/aaron/Zotero/storage/7AAZ82ZJ/Piccolo and Frampton - 2016 - Tools and techniques for computational reproducibi.pdf;/home/aaron/Zotero/storage/LVTBIAVR/s13742-016-0135-4.html}
}

@inproceedings{claerboutElectronicDocumentsGive1992,
  langid = {english},
  title = {Electronic Documents Give Reproducible Research a New Meaning},
  url = {http://library.seg.org/doi/abs/10.1190/1.1822162},
  doi = {10.1190/1.1822162},
  eventtitle = {{{SEG Technical Program Expanded Abstracts}} 1992},
  booktitle = {{{SEG Technical Program Expanded Abstracts}} 1992},
  publisher = {{Society of Exploration Geophysicists}},
  urldate = {2019-08-29},
  date = {1992-01},
  pages = {601-604},
  author = {Claerbout, Jon F. and Karrenbach, Martin}
}

@report{herouxCompatibleReproducibilityTaxonomy2018,
  langid = {english},
  title = {Toward a {{Compatible Reproducibility Taxonomy}} for {{Computational}} and {{Computing Sciences}}.},
  url = {https://www.osti.gov/biblio/1481626},
  abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
  number = {SAND2018-11186},
  institution = {{Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)}},
  urldate = {2019-08-29},
  date = {2018-10-01},
  author = {Heroux, Michael A. and Barba, Lorena and Parashar, Manish and Stodden, Victoria and Taufer, Michela},
  doi = {10.2172/1481626}
}


@report{stoddenEnablingReproducibleResearch2009,
  langid = {english},
  location = {{Rochester, NY}},
  title = {Enabling {{Reproducible Research}}: {{Open Licensing}} for {{Scientific Innovation}}},
  url = {https://papers.ssrn.com/abstract=1362040},
  shorttitle = {Enabling {{Reproducible Research}}},
  abstract = {There is a gap in the current licensing and copyright structure for the growing number of scientists releasing their research publicly, particularly on the Internet. Scientific research produces more scholarship than the final paper: for example, the code, data structures, experimental design and parameters, documentation, and figures, are all important both for communication of the scholarship and replication of the results. US copyright law is a barrier to the sharing of scientific scholarship since it establishes exclusive rights for creators over their work, thereby limiting the ability of others to copy, use, build upon, or alter the research. This is precisely opposite to prevailing scientific norms, which provide both that results be replicated before accepted as knowledge, and that scientific understanding be built upon previous discoveries for which authorship recognition is given. In accordance with these norms and to encourage the release of all scientific scholarship, I propose the Reproducible Research Standard (RRS) both to ensure attribution and facilitate the sharing of scientific works. Using the RRS on all components of scientific scholarship will encourage reproducible scientific investigation, facilitate greater collaboration, and promote engagement of the larger community in scientific learning and discovery.},
  number = {ID 1362040},
  institution = {{Social Science Research Network}},
  type = {SSRN Scholarly Paper},
  urldate = {2019-08-29},
  date = {2009-03-03},
  keywords = {Enabling Reproducible Research: Open Licensing for Scientific Innovation,SSRN,Victoria Stodden},
  author = {Stodden, Victoria},
  file = {/home/aaron/Zotero/storage/MXUU9SXE/Stodden - ENABLING REPRODUCIBLE RESEARCH OPEN LICENSING FOR.pdf;/home/aaron/Zotero/storage/AJA3GYX7/papers.html}
}


@Manual{jetpack,
  title = {jetpack: A Friendly Package Manager},
  author = {Andrew Kane},
  year = {2019},
  note = {R package version 0.4.3},
  url = {https://github.com/ankane/jetpack},
}

@Manual{miniCRAN,
  title = {miniCRAN: Create a Mini Version of CRAN Containing Only Selected Packages},
  author = {Andrie {de Vries}},
  year = {2019},
  note = {R package version 0.2.12},
  url = {https://CRAN.R-project.org/package=miniCRAN},
}

@Manual{checkpoint,
  title = {checkpoint: Install Packages from Snapshots on the Checkpoint Server for
Reproducibility},
  author = {Microsoft Corporation},
  year = {2019},
  note = {R package version 0.4.6},
  url = {https://CRAN.R-project.org/package=checkpoint},
}

@article{marwickPackagingDataAnalytical2018,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  volume = {72},
  issn = {0003-1305},
  url = {https://amstat.tandfonline.com/doi/full/10.1080/00031305.2017.1375986},
  doi = {10.1080/00031305.2017.1375986},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  number = {1},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  urldate = {2019-08-28},
  date = {2018-01-02},
  pages = {80-88},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  file = {/home/aaron/Zotero/storage/QRGL5MLD/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using .pdf;/home/aaron/Zotero/storage/CSPY6R87/00031305.2017.html}
}


@Manual{boot,
    title = {boot: Bootstrap R (S-Plus) Functions},
    author = {Angelo Canty and B. D. Ripley},
    year = {2019},
    note = {R package version 1.3-23},
  }

 @Article{lavaan2012,
    title = {{lavaan}: An {R} Package for Structural Equation Modeling},
    author = {Yves Rosseel},
    journal = {Journal of Statistical Software},
    year = {2012},
    volume = {48},
    number = {2},
    pages = {1--36},
    url = {http://www.jstatsoft.org/v48/i02/},
  }

@article{knuth1984literate,
  title={Literate programming},
  author={Knuth, Donald Ervin},
  journal={The Computer Journal},
  volume={27},
  number={2},
  pages={97--111},
  year={1984},
  publisher={Oxford University Press}
}

@article{epskamp2019rep,
  author = {Sacha Epskamp},
  title ={Reproducibility and Replicability in a Fast-Paced Methodological World},
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {145-155},
  year = {2019},
  doi = {10.1177/2515245919847421},
  URL = {https://doi.org/10.1177/2515245919847421},
  eprint = {https://doi.org/10.1177/2515245919847421},
  abstract = { Methodological developments and software implementations are progressing at an increasingly fast pace. The introduction and widespread acceptance of preprint archived reports and open-source software have made state-of-the-art statistical methods readily accessible to researchers. At the same time, researchers are increasingly concerned that their results should be reproducible (i.e., the same analysis should yield the same numeric results at a later time), which is a basic requirement for assessing the results’ replicability (i.e., whether results at a later time support the same conclusions). Although this age of fast-paced methodology greatly facilitates reproducibility and replicability, it also undermines them in ways not often realized by researchers. This article draws researchers’ attention to these threats and proposes guidelines to help minimize their impact. Reproducibility may be influenced by software development and change over time, a problem that is greatly compounded by the rising dependency between software packages. Replicability is affected by rapidly changing standards, researcher degrees of freedom, and possible bugs or errors in code, whether introduced by software developers or empirical researchers implementing an analysis. This article concludes with a list of recommendations to improve the reproducibility and replicability of results.}
}

@misc{dfg2019,
  langid = {german},
  title = {Leitlinien zur Sicherung guter wissenschaftlicher Praxis},
  url = {https://www.dfg.de/download/pdf/foerderung/rechtliche_rahmenbedingungen/gute_wissenschaftliche_praxis/kodex_gwp.pdf},
  date = {2019},
  author = {{Deutsche Forschungsgemeinschaft}}
}

@Manual{xie2019,
  title = {knitr: A General-Purpose Package for Dynamic Report Generation in R},
  author = {Yihui Xie},
  year = {2019},
  note = {R package version 1.22},
  url = {https://yihui.name/knitr/},
}

@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {https://yihui.name/knitr/},
}

@Manual{papaja2018,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2018},
  note = {R package version 0.1.0.9842},
  url = {https://github.com/crsh/papaja},
}

@Manual{stargazer2018,
  title = {stargazer: Well-Formatted Regression and Summary Statistics Tables},
  author = {Marek Hlavac},
  year = {2018},
  note = {R package version 5.2.2},
  organization = {Central European Labour Studies Institute (CELSI)},
  address = {Bratislava, Slovakia},
  url = {https://CRAN.R-project.org/package=stargazer},
}
