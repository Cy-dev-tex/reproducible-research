---
title: "A Reproducible Workflow with RMarkdown, Git, Make & Docker"
shorttitle: "Reproducible Workflow"
author: "Andreas Brandmaier & Aaron Peikert"
date: "8/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("pacman"))install.packages("pacman")
pacman::p_load("here", "tidyverse")
```

In this tutorial, we describe a principled workflow to ensure reproducible research in R-based environments. The workflow relies on the interplay of various open-source software tools including Rmarkdown, git, make and docker whose interplay ensures a seamless integration of version management, (APA-conformable) dynamic report generation and full cross-platform computational reproducibility. The workflow ensures the primary goals that 1) the reporting of analysis results is perfectly consistent with the actual analysis results (dynamic report generation), the analysis exactly reproduces at later time even if the computing and software platform is updated or changed (computational reproducibility), and 3) intermediate or post-publication changes are tracked, marked, and documented while earlier versions of both data and code remain accessible. While dynamic document generation is increasingly recognized as tool for reproducible analyses, we demonstrate with practical examples that dynamic documents are not sufficient to ensure computational reproducibility.

Reproducible Research has become the gold standard for scientific work in recent years. The following tutorial aims to outline a workflow that does guarantees not only a reasonable standard of reproducibility but also decreases the burden of maintaining software for the researcher and his colleagues. The authors' believe that integrating tools from software engineering in the process of scientific reporting to enhance reproducibility is crucial and give guidelines for the practitioner to use them. These tools include software for literate programming, version control, dependency tracking, and containerization.

Literate Programming
: Interwoven text and computer code that together forms a human-readable document.
: Example: One document that includes text that describes a statistical test, code that is used to execute the statistical and the output of the code in the same document.
Version Control
: The management of change of documents/code.
: Example: Changes from the manuscript text upon request by the reviewer are identifiable. 

Dependency Tracking
: How different parts of a manuscript/analysis relate and how these dependencies resolve via execution of code.
: Example: The manuscript depends on the statistical analysis, which depends on the data.

Containerization
: A virtual computational environment that does encapsulate all needed software. More formally called operating-system-level virtualization.
: Example: A statistical analysis depends on the used statistical software and a virtual machine bundles them together.

Within the community of users of the R Programming Language for each purpose a specific software solutions is clearly the most popular. For literate programming, `rmarkdown`, for version control `git`, for dependency management `make` and for container `docker`. Each of them serves a meta-scientific valuable goal (reproducibility) and increases the producitivity of the researcher. Each of these software solutions are extremly mighty, so in order to master them fully often years of practice are required. Luckily the minimal valuable subset needed to ensure reproducibility can be learned with little practice.

The authors believe that for sufficient chances of reproducibility, good documentation is crucial, since it anwers the question "What steps have to be taken to reproduce the results?". The most precise documentation is computer code and the most productive computer code is code that runs automaticly without human interaction. These principles are what guides the the use of `rmarkdown`, `git`, `make`, `docker` and there interaction.

The first step towards reproducibility is to have a R script, that can be executed on the own computer without error, after everything within R and the files that are the output of the script deleted. We assume that the reader is familar with R and has the ability to archive that for a simple analysis. A next step is to make sure that all files relevant, can be moved to another computer. To archive that it is important that all files are within one folder (and subfolders within it) and all paths are relative to that folder. A nice solution to that problem are [Rstudio Projects](https://r4ds.had.co.nz/workflow-projects.html) and/or the `here` package.

```{r, eval=FALSE}
# BAD
iris <- read.csv("/home/aaron/Documents/reproducible-research/data/iris.csv")
# GOOD
iris <- read.csv("data/iris.csv")
# BETTER
iris <- read.csv(here("data", "iris.csv"))
```

This folder, where everything resides that you need for analysis, is referred to as a Projekt. Working with Projects is exceptionally comfortable with RStudio an integrated development environment (IDE) for R.

Reproducibility at a basic level assumes that results are the same if neither script nor data have changed. It is often not trivial to find out whether anything has changed and if so to "go back in time." `git` enables you to do both. A good mental model for `git` is that it takes a sequence of snapshots of all files it is supposed to track. In the language of git, these snapshots are "commits".  A commit represents a complete copy of the state of the files when added to a commit. Each commit has a unique identifier (a hash). Going back to one state is as easy as finding the hash of the commit. This can be made even easier by tagging specific commits e.g., as "submission," "preprint," "publication." This collection of commits is called "repository," which is almost the same as your Project.

A typical git workflow looks like that:

```{bash, eval=FALSE}
git init # to initialize git
git add /data/iris.csv /R/analysis.R # track specific files/changes
git commit -m "add data & analysis" # take snapshot
# new data comes in
git commit -a -m "complete data colections" # add (-a) and comit all changes
```

So to keep track of all changes, you only need to use `git add` & `git commit` or `git commit -a` to do both. These commands need to be executed in the `terminal`, which you can access from within Rstudio (`Shift + Alt + R`), but if you use Git with Rstudio, you also get a graphical user interface and quite likely you won't need the terminal more then once per project.

![Git Pane in RStudio](Images/git-pane.png)

Now you can check everything that has happened (`git log`) and look at the exact state by supplying the id of the commit or the first few digits of it to `git checkout`:

```{bash, eval = FALSE}
git log
git checkout 77db06f78e
```

Git also makes it especially easy to share and collaborate on a project with another researcher. A popular service for sharing via git is [Github](https://github.com). Just sharing Git repositories with the public is always free, private repositories (only visible to persons you invite) are [free for researcher](https://help.github.com/en/articles/applying-for-an-educator-or-researcher-discount). After creating a user account, you can create a new repository where Github advises you how to upload your repository from the terminal e.g., for this repository:

```{bash, eval=FALSE}
git remote add origin git@github.com:aaronpeikert/reproducible-research.git
git push -u origin master
```

`git push` or the green up arrow in the git pane uploads local updates. To download this git repo to another computer type into the terminal:

```{bash, eval=FALSE}
git clone git@github.com:aaronpeikert/reproducible-research.git
```

Git & Github can do even more to help you to collaborate with your fellow researcher, but this is beyond the already broad scope of this tutorial. Another benefit of using Git & Github is that experimentation is highly encouraged since you can go back to any state quickly and even when you lose access to the file on your computer, everything is backed up on the Github servers.

Even when you have obtained a version of the project, and you can ensure that this version is unchanged, you may not know how to reproduce the results, because it is unclear what has to be run in which order. This job gets comfortable with `make`, since it allows you to create (computational) recipes too (re-)create files.

A `Makefile` obeys the following scheme:

```
target: dependency1 dependency2
  command-to-create-target
```

A typical `Makefile` might look like:

```
analysis.pdf: data/clean.csv
  Rscript -e 'rmarkdown::render("analysis.Rmd")'

data/clean.csv: R/clean.R data/iris.csv
  Rscript -e 'source("R/clean.R")'
```

The first line reads: to create `analysis.pdf` the `analysis.Rmd` needs to be rendered, which depends on `data/clean.csv`. This dependency is itself a target. To create `data/clean.csv`, `R/clean.R` and `data/iris.csv` are needed. If you type `make analysis.pdf` `make` first checks whether the dependencies do exist and if not whether or it knows how to create them. So if `data/clean.csv` does not exist, `make` creates it. The same thing happens if one of the dependencies of a target is newer then the target itself, then updates everything that directly or indirectly depends on it. So if `data\iris.csv` is newer then `data\clean.csv`, make attempts to recreate `data\clean.csv` and `analysis.pdf`. If there is a dependency which is missing, and there is no recipe to make it, `make` stops with an error. It is a convention to have a target `all` which depends on everything and make it the first target in a `Makefile`, then the command `make` without any argument automatically creates everything. `Build All` from within RStudio also triggers this process.

```
all: analysis.pdf
```

![Build Pane in RStudio](Images/build-pane.png)

If you have followed the above workflow a fellow researcher is three commands away from fully reproducing your analysis:

```{bash, eval=FALSE}
git clone https://github.com/sirfisher/iris.git
cd iris
make all
```

However, this relies on the crucial assumption that your computational environments are sufficiently compatible e.g., everything needed is installed (R + Packages) at a compatible version of the software.

### Random Number Generation with R


```{bash}
docker run --rm rocker/r-ver:3.5.0 Rscript -e "set.seed(1234);sample(1:10, 5)"
```

```{bash}
docker run --rm rocker/r-ver:3.6.0 Rscript -e "set.seed(1234);sample(1:10, 5)"
```
